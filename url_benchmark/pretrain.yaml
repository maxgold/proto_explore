defaults:
  - agent: ddpg
  - override hydra/launcher: submitit_slurm

# mode
reward_free: true
# task settings
domain: walker # primal task will be infered in runtime
task: point_mass_maze_reach_custom_goal
task_no_goal: point_mass_maze_reach_no_goal
obs_type: states # [states, pixels]
frame_stack: 3 # only works if obs_type=pixels
action_repeat: 2 # set to 2 for pixels
discount: .99
# train settings
num_train_frames: 2000010
num_seed_frames: 4000
# eval
eval: False
eval_every_frames: 200000
num_eval_episodes: 2
# snapshot
snapshots: [100000, 500000, 1000000, 2000000]
snapshot_dir: ../../../models/${obs_type}/${domain}/${task}/${agent.name}/seed${seed}/offset${offset}/stdsch2${stdsch2}/stdclip2${stdclip2}/
# replay buffer
replay_buffer_size: 500000
replay_buffer_num_workers: 4
nstep: ${agent.nstep}
update_encoder: true # should always be true for pre-training
# misc
seed: 1
device: cuda
save_video: true
save_train_video: false
use_tb: false
use_wandb: false
goal: false
hybrid: false
hybrid_gc: false
hybrid_pct: .5 #0-1
batch_size: 1024
batch_size_gc: 1024
hidden_dim: 1024
update_gc: 2
replay_buffer_gc: 100000
lr: .0001
offline: False
tmux_session: ???
test: False
switch_gc: 100000         #when to switch from proto pretrain to gc if training hybrid (proto+gc)
load_encoder: False
load_proto: False
const_init: True
eval_after_step: 990000
episode_length: 500
num_protos: 512
episode_reset_length: 500
stddev_schedule: .2
stddev_schedule2: .2
stddev_clip: .3
stddev_clip2: .3
use_predictor: False
load_model: False
pred_dim: 128
proj_dim: 512
feature_dim: 50
num_iterations: 3
tau: .1
sl: False
asym: False
goal_num: 20
update_proto_every: 2
loss: False                     # adds a random_obs to your batched data, chosen by idx = np.random.randint(0, episode_len(episode))
debug: False
combine_storage_gc: False
proto_goal_intr: False
proto_goal_random: True
loss_gc: False                  #like loss, except for the gc loader 
update_enc_proto: False
update_enc_gc: False
ot_reward: False
neg_euclid: False
actionable: False
reward_cutoff: 0
neg_euclid_state: True
nstep1: 3
nstep2: 3
update_proto_opt: True
update_gc_while_proto: False
update_proto_while_gc: False
pmm_reward_cutoff: 100
model_path: False
cassio: False       #when model_path is not None; gets the exact path based on machine
greene: False       # .
pluto: False        # .
irmak: False        # .
normalize: False
normalize2: True
load_every: 10000
offline_gc: False   # if gc_only then need to set init_from_proto or init_from_ddpg, model_path ...; else ...
goal_offset: 1
gc_only: False
log_every_steps: 1000
model_step_index: -1
proto_only: False
resume_training: False
encoder1: False
encoder2: False
encoder3: False
feature_dim_gc: 50
inv: False
use_actor_trunk: False
use_critic_trunk: False
expert_buffer: False
offline_model_step: None
offline_model_step_lb: None
init_from_proto: False
init_from_ddpg: False
pretrained_feature_dim: 16
buffer_num: 0
reverse: True
velocity_control: False
scale: None
egocentric: ???
camera_id: ??? #quadruped=2 egocentric=1
proto_explore_every: 10
proto_explore_episodes: 100
hack: False
eval_proto_goals: False
num_gc_train_frames: 100000
num_proto_train_frames: 100000
eval_episode_length: 50
#experiment
experiment: exp

path: /home/ubuntu/proto_explore/url_benchmark/models/eval_8_23_4
replay_dir: /home/ubuntu/proto_explore/url_benchmark/models/eval_8_23_4/buffer1/buffer_copy
replay_dir2: false

hydra:
  run:
    dir: ./exp_local/${now:%Y.%m.%d}/${now:%H%M%S}_${agent.name}_offset${goal_offset}_stds${stddev_schedule2}_stdc${stddev_clip2}_seed${seed}
  sweep:
    dir: ./exp_sweep/${now:%Y.%m.%d}/${now:%H%M}_${agent.name}_${experiment}
    subdir: ${hydra.job.num}
  launcher:
    timeout_min: 4300
    cpus_per_task: 16
    gpus_per_node: 1
    tasks_per_node: 1
    mem_gb: 200
    nodes: 1
    submitit_folder: ./exp_sweep/${now:%Y.%m.%d}/${now:%H%M}_${agent.name}_update${update_gc}_bs${batch_size_gc}_hidden${hidden_dim}_${experiment}/.slurm
